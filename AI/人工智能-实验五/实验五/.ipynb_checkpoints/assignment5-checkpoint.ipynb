{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = black size=6>实验五:随机森林</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实验为编写集成学习中的随机森林算法。在上一次实验中，我们已经学会了如何构建一棵ID3决策树。在本次实验，我们将以上一次决策树代码的基础上，结合集成学习中的并行化生成分类模型的思想，构建多棵决策树，组成随机森林。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第一部分:函数介绍</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "介绍一些在数据采样和属性集采样的过程中可以用到的随机函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 1 2 3 1 3 4 2 3]\n",
      "   0  1  2\n",
      "3  3  3  3\n"
     ]
    }
   ],
   "source": [
    "# np.random.choice函数从一个一维数组中随机采样\n",
    "x = np.array([1,2,3,4])\n",
    "y = np.random.choice(x, replace=True, size=10)\n",
    "print(y)\n",
    "\n",
    "# np.random.shuffle函数对一个数组/矩阵按照第一维进行洗牌\n",
    "x = np.array([[0,1,2],[3,4,5],[6,7,8],[9,10,11],[12,13,14]])\n",
    "np.random.shuffle(x)\n",
    "# print(x)\n",
    "\n",
    "# DataFrame对象的sample函数可以随机采样n个数据或者采样比例为frac的数据\n",
    "x = np.array([[0,0,0],[1,1,1],[2,2,2],[3,3,3],[4,4,4]])\n",
    "frame = pd.DataFrame(x)\n",
    "print(frame.sample(n=1))\n",
    "# print(frame.sample(frac=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第二部分:实验任务</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实验承接上次实验，实现随机森林。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">1) 采用自助采样法对训练数据集'train_titanic.csv'进行采样，生成$n$个训练数据集($n$自行设定)。自助采样法是指，每次从原本数据集中【有放回】地随机采样一个数据，重复进行$m$次，就生成一个有$m$个数据的训练数据集($m$是原本数据集的数据个数)。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame = pd.read_csv('train_titanic.csv')\n",
    "train=np.array(train_frame)\n",
    "# Bootstrap 采样\n",
    "np.random.shuffle(train)\n",
    "\n",
    "#生成一个具有m个数据的训练数据集\n",
    "def m_data(train):\n",
    "    a = np.arange(0, len(train), 1)\n",
    "    b = np.random.choice(a, replace=True, size=len(train))\n",
    "    data: list = []\n",
    "    for i in b:\n",
    "        p: list = []\n",
    "        p = train[i, :]\n",
    "        data.append(p)\n",
    "    m_data = np.array(data)\n",
    "    return m_data\n",
    "\n",
    "new_train=[]\n",
    "i=0\n",
    "while i<10:\n",
    "    new_train.append(m_data(train))\n",
    "    i+=1\n",
    "new_train=np.array(new_train)\n",
    "# print(new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">2) 对上次实验的best_split函数进行修改，改成先从属性集$A$中先随机选取$k$个属性构成属性集$A'$，再从$A'$中选取最佳划分的属性。($k$是一个整数，一般取$max(round(log_2 d),1)$, 其中$d$是$A$的元素的个数)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算信息熵\n",
    "def entropy(label):\n",
    "    all = np.unique(label[:])\n",
    "    counter = Counter(label[:])\n",
    "    ent = 0\n",
    "    for a in all:\n",
    "        pk = counter[a] / len(label)\n",
    "        ent -= pk * math.log((pk), 2)\n",
    "    return ent\n",
    "\n",
    "#将属性集合按照某一维度进行划分\n",
    "def split(feature, label, dimension):\n",
    "    dim = feature[:, dimension]    #取出属性集中被划分的维度\n",
    "    differ = np.unique(dim)        # differ为被划分属性上某一维度的非重复值数组\n",
    "    split_feature = []             #创建一个列表存储被划分的数据集\n",
    "    split_label = []               #创建一个列表存储被划分后的标签集\n",
    "    #遍历dimension维度上的非重复值\n",
    "    for x in differ:\n",
    "        small_feature = []\n",
    "        small_label = []\n",
    "        for i in range(len(dim)):\n",
    "            if (x == dim[i]):\n",
    "                q: list = feature[i, :]\n",
    "                w: list = label[i]\n",
    "                small_feature.append(q)\n",
    "                small_label.append(w)\n",
    "        split_feature.append(small_feature)\n",
    "        split_label.append(small_label)\n",
    "    return split_feature, split_label\n",
    "\n",
    "\n",
    "def best_split_1(D, A):\n",
    "    label:list=D[:,len(D[0,:])-1]\n",
    "    best_entropy = -1\n",
    "    best_dimension = 0\n",
    "    for col in A:\n",
    "        d= entropy(label)\n",
    "        sp_feature, sp_label = split(D, label, col)\n",
    "        d_v=0\n",
    "        i=0\n",
    "        for arr in sp_label:\n",
    "            d_v -= len(arr) / len(label) * entropy(sp_label[i])\n",
    "            i+=1\n",
    "        if d_v + d > best_entropy:\n",
    "            best_entropy = d_v + d\n",
    "            best_dimension = col\n",
    "    return best_dimension\n",
    "\n",
    "#在集合A中随机选取几个元素组成新的集合再按照新的集合调用benst_split_1函数，得到最佳划分维度\n",
    "def best_split(D, A):\n",
    "    Aa= np.random.choice(A, replace=True, size=max(round(math.log(len(A))),1))\n",
    "    return best_split_1(D,Aa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">3) 对上次实验完成的决策树类进行如下修改：①predict函数不需要计算预测准确率，要返回数据集D的预测标签；②每个属性的可能取值possible_value不从采样的数据集中取，而是从原本数据集'train_titanic.csv'中取，以防止在预测过程中出现决策树在训练过程中未见过的属性取值。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记下所有属性可能的取值\n",
    "D = np.array(train_frame)\n",
    "A = set(range(D.shape[1] - 1))\n",
    "possible_value = {}\n",
    "for every in A:\n",
    "    possible_value[every] = np.unique(D[:, every])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 树结点类\n",
    "class Node:\n",
    "    def __init__(self, isLeaf=False, label=-1, index=-1):\n",
    "        self.isLeaf = isLeaf # isLeaf表示该结点是否是叶结点\n",
    "        self.label = label # label表示该叶结点的label（当结点为叶结点时有用）\n",
    "        self.index = index # index表示该分支结点的划分属性的序号（当结点为分支结点时有用）\n",
    "        self.children = {} # children表示该结点的所有孩子结点，dict类型，方便进行决策树的搜索\n",
    "        \n",
    "    def addNode(self, val, node):\n",
    "        self.children[val] = node #为当前结点增加一个划分属性的值为val的孩子结点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">4) 生成$n$棵决策树实例，每棵决策树对上面生成的$n$个训练数据集中的一个数据集进行训练。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树类\n",
    "class DTree:\n",
    "    def __init__(self):\n",
    "        self.tree_root = None #决策树的根结点\n",
    "        self.possible_value = possible_value # 用于存储每个属性可能的取值\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    TreeGenerate函数用于递归构建决策树，伪代码参照课件中的“Algorithm 1 决策树学习基本算法”\n",
    "    '''\n",
    "    def TreeGenerate(self, D1, A):\n",
    "        D=np.array(D1)\n",
    "        # 生成结点 node\n",
    "        node = Node()\n",
    "        #取出最后一列作为标签集合\n",
    "        label:list = D[:,len(D[0, :]) - 1]\n",
    "        label_arr = np.array(label)\n",
    "\n",
    "        # if D中样本全属于同一类别C then\n",
    "        #     将node标记为C类叶结点并返回\n",
    "        # end if\n",
    "        if (1 == len(np.unique(label_arr[:]))):\n",
    "            node.label = np.unique(label_arr)[0]\n",
    "            node.isLeaf=True\n",
    "            return node\n",
    "        # if A = Ø OR D中样本在A上取值相同 then\n",
    "        #     将node标记叶结点，其类别标记为D中样本数最多的类并返回\n",
    "        # end if\n",
    "        label_mark = -1    #D中样本数量最多的标签类\n",
    "        Flag = True #当按照A中所存的维度进行索引属性值的时候，所有索引出来的属性值相同时flag的值为true\n",
    "        for i in A:\n",
    "            col=D[:,i]\n",
    "            col_differ=np.unique(col)\n",
    "            if len(col_differ)>1:\n",
    "                Flag=False\n",
    "        MAX = -1\n",
    "        differ_label = np.unique(label[:])\n",
    "        counter = Counter(label[:])\n",
    "        for l in differ_label:\n",
    "            num = counter[l]\n",
    "            if (num > MAX):\n",
    "                MAX = num\n",
    "                label_mark = l\n",
    "        if (len(A) == 0 or Flag):\n",
    "            node.label = label_mark\n",
    "            node.isLeaf=True\n",
    "            return node\n",
    "\n",
    "        # 从A中选择最优划分属性a_star\n",
    "        # （选择信息增益最大的属性，用到上面实现的best_split函数）\n",
    "        # a_star = best_split(D, A)\n",
    "        a_star = best_split(D, A)\n",
    "        node.index=a_star\n",
    "        # for a_star 的每一个值a_star_v do\n",
    "        #     为node 生成每一个分支；令D_v表示D中在a_star上取值为a_star_v的样本子集\n",
    "        #     if D_v 为空 then\n",
    "        #         将分支结点标记为叶结点，其类别标记为D中样本最多的类\n",
    "        #     else\n",
    "        #         以TreeGenerate(D_v,A-{a_star}) 为分支结点\n",
    "        #     end if\n",
    "        # end for\n",
    "        # print(\"a_star=\",a_star)\n",
    "        a_dimension = D[:, a_star]\n",
    "        # a_strat_differ = np.unique(a_dimension[:])\n",
    "        a_strat_differ=self.possible_value[a_star]\n",
    "        for d in a_strat_differ:\n",
    "            lis: list = []\n",
    "            for i in range(len(D[:, 0])):\n",
    "                if (a_dimension[i] == d):\n",
    "                    tem:list=D[i,:]\n",
    "                    lis.append(tem)\n",
    "            if len(lis)==0:\n",
    "                new_nd=Node()\n",
    "                new_nd.isLeaf=True\n",
    "                new_nd.label=label_mark\n",
    "                node.addNode(d,new_nd)\n",
    "            else:\n",
    "                B: list = []\n",
    "                for x in A:\n",
    "                    if x == a_star:\n",
    "                        continue\n",
    "                    else:\n",
    "                        B.append(x)\n",
    "                new_node: Node = self.TreeGenerate(lis,B)\n",
    "                node.addNode(d, new_node)\n",
    "        return node\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    train函数可以做一些数据预处理（比如Dataframe到numpy矩阵的转换，提取属性集等），并调用TreeGenerate函数来递归地生成决策树\n",
    "    '''\n",
    "    def train(self, D):\n",
    "        D = np.array(D)  # 将Dataframe对象转换为numpy矩阵（也可以不转，自行决定做法）\n",
    "        # A = set(range(D.shape[1] - 1)) # 属性集A\n",
    "        A: list = list(range(len(D[0, :]) - 1))\n",
    "        # 记下每个属性可能的取值\n",
    "        # A = np.array(A)\n",
    "        self.tree_root = self.TreeGenerate(D, A)  # 递归地生成决策树，并将决策树的根结点赋值给self.tree_root\n",
    "\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    predict函数对测试集D进行预测，输出预测标签\n",
    "    '''\n",
    "    def search(self, node: Node, arr):\n",
    "        if node.isLeaf:\n",
    "            return node.label\n",
    "        else:\n",
    "            x = arr[node.index]\n",
    "            next1 = node.children[x]\n",
    "            return self.search(next1, arr)\n",
    "    \n",
    "    \n",
    "    def predict(self, D):\n",
    "        D=np.array(D)\n",
    "        label:list=[]\n",
    "        for i in range(len(D[:,0])):\n",
    "            label.append(self.search(self.tree_root,D[i,:]))\n",
    "        return label\n",
    "#         D = np.array(D) # 将Dataframe对象转换为numpy矩阵（也可以不转，自行决定做法）\n",
    "        \n",
    "#         #对于D中的每一行数据d，从当前结点x=self.tree_root开始，当当前结点x为分支结点时，\n",
    "#         #则搜索x的划分属性为该行数据相应的属性值的孩子结点（即x=x.children[d[x.index]]），不断重复，\n",
    "#         #直至搜索到叶结点，该叶结点的label就是数据d的预测label\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Your code here -------\n",
    "\n",
    "#创建一个senl列表，存储训练好的10个决策树模型\n",
    "senl:list=[]\n",
    "#引入test测试集\n",
    "test_frame=pd.read_csv('test_titanic.csv')\n",
    "test=np.array(test_frame)\n",
    "\n",
    "#在循环中生成决策树，并将训练好的模型放到列表senl中去\n",
    "j=0\n",
    "while j<100:\n",
    "    my_data=m_data(train)   #随机抽取与train个数相同的数据组成新的数据集\n",
    "    dt=DTree()\n",
    "    dt.train(my_data)    #使用随机生成的数据集来训练决策树\n",
    "    senl.append(dt)      #将训练好的决策树放到列表中\n",
    "    j+=1\n",
    "\n",
    "#用列表yuce来存储每一个决策树预测出来的结果\n",
    "yuce:list=[]\n",
    "#遍历每一个决策树，将每次预测的结果作为一个一维数组，最终所有决策树结果构成一个二维数组\n",
    "for dt in senl:\n",
    "    lis:list=dt.predict(test)\n",
    "    yuce.append(lis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">5) 用训练完成的$n$棵决策树分别对测试数据集'test_titanic.csv'进行预测。采用相对多数投票法$H(x)=C_{\\mathop{\\arg\\max}_{j} \\sum_{i=1}^T h_i^j(x)}$来对各棵决策树的预测结果进行结合。输出结合的预测结果的准确率。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机森林最终的预测结果为:\n",
      " [0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "预测正确率为： 0.8366336633663366\n"
     ]
    }
   ],
   "source": [
    "test_frame = pd.read_csv('test_titanic.csv')\n",
    "\n",
    "# ----- Your code here -------\n",
    "pre=[]\n",
    "num=0\n",
    "yuce=np.array(yuce)\n",
    "#遍历二维数组的每一列，将每一列中出现次数最多的类别作为这一列的最终结果，并计算预测正确率\n",
    "for i in range(len(yuce[0,:])):\n",
    "    col=yuce[:,i]\n",
    "    differ=np.unique(col[:])\n",
    "    counter=Counter(col[:])\n",
    "    max1=0\n",
    "    label=0\n",
    "    for x in differ:\n",
    "        if counter[x] >max1:\n",
    "            max1=counter[x]\n",
    "            label=x\n",
    "    pre.append(label)\n",
    "    if label==test[i,4]:\n",
    "        num+=1\n",
    "print(\"随机森林最终的预测结果为:\\n\",pre)\n",
    "print(\"预测正确率为：\",num/len(test[:,0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第三部分:作业提交</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、实验课下课前提交完成代码，如果下课前未完成，请将已经完成的部分进行提交，未完成的部分于之后的实验报告中进行补充  \n",
    "要求:  \n",
    "1)文件格式为：学号-姓名.ipynb  \n",
    "2)【不要】提交文件夹、压缩包、数据集等无关文件，只需提交单个ipynb文件即可，如果交错请到讲台前联系助教，删掉之前的错误版本后再进行提交"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、因为下周放假冲了一次理论课，本次实验做两周，实验报告下下周（4月15号前）交  \n",
    "要求：  \n",
    "1)文件格式为：学号-姓名.pdf  \n",
    "2)【不要】提交文件夹、压缩包、代码文件、数据集等任何与实验报告无关的文件，只需要提交单个pdf文件即可  \n",
    "3)文件命名时不需要额外添加“实验几”等额外信息，按照格式提交  \n",
    "4)每周的实验报告提交地址会变化，且有时间限制，提交时间为下周的实验课开始时，请注意及时提交。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验五(随机森林)的实验报告上交地址:https://workspace.jianguoyun.com/inbox/collect/3c4acbb1e9a044c48fec14e2fdb97b56/submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、课堂课件获取地址:https://www.jianguoyun.com/p/DQlpUFYQp5WhChiS_q0E  \n",
    "实验内容获取地址:https://www.jianguoyun.com/p/DbKbP-AQp5WhChi1sa0E"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
